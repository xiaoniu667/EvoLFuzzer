# **EvoLFuzzer - LLM Code Fuzzing Framework Based on Evolutionary Algorithms**
ğŸŒ **Languages**: [English](README.md) | [ä¸­æ–‡](README_zh.md)

## ğŸ“˜ Overview
**EvoLFuzzer** is a white-box fuzzing framework that combines **Evolutionary Algorithms (EAs)** with **Large Language Model (LLM)-guided semantic analysis**, designed to systematically evaluate and enhance the security of code generated by LLMs.  
By leveraging **code semantic understanding + coverage feedback + seed evolution strategies**, the framework can significantly improve path coverage and vulnerability detection on multiple open-source benchmark datasets.

The project provides:

+ Full implementation of EvoLFuzzer
+ Reproducible experimental environment and configurations
+ Multiple baseline methods and comparison experiments
+ Evaluation modules including path coverage and vulnerability detection metrics
+ Integrated ablation studies
+ Parameter sensitivity experiments

---

## ğŸ”¥ Key Features
+ ğŸš€ **Evolutionary Fuzzing**: Dynamically generates and optimizes test cases through multiple rounds of evolutionary strategies
+ ğŸ¤– **LLM Guidance**: Integrates DeepSeek-V3.2-Exp (deepseek-chat) for code generation and semantic mutation
+ ğŸ“Š **Coverage-Driven Feedback**: Uses Coverage tools to collect path coverage data
+ ğŸ”’ **Security-Oriented Testing**: Focused on identifying potential security vulnerabilities in LLM-generated code
+ ğŸ“ˆ **Multi-Round Evolution Analysis**: Supports visualization of coverage trends and iterative behavior

---

## ğŸ–¥ Experimental Environment
### **Hardware**
+ **CPU**: Intel Core i9-10900X (10 cores / 20 threads)
+ **Memory**: 128 GB RAM
+ **OS**: Ubuntu 22.04 LTS (64-bit)

### **Software Dependencies**
+ **LLM Model**: DeepSeek-V3.2-Exp
+ **Coverage Tool**: Coverage
+ **Python**: 3.10

### **Experimental Settings**
+ Initial number of seeds: **10**
+ Number of evolution rounds: **5**
+ Number of repeated experiments: **5 (average results)**

---

## ğŸ“š Supported Datasets
EvoLFuzzer is evaluated on the following five widely used natural language code generation datasets:

1. **HumanEval** â€“ Manually crafted programming problem set
2. **LLMSecEval** â€“ Focused on evaluating secure code generation by LLMs
3. **MBPP** â€“ Small Python programming tasks provided by Google
4. **CWEval** â€“ Security vulnerability dataset based on Common Weakness Enumeration (CWE)
5. **SecurityEval** â€“ Code generation evaluation dataset for software security scenarios

---

## ğŸš€ Usage
### 1. **Installation**
```plain
# Clone the repository
git clone https://github.com/xiaoniu667/EvoLFuzzer.git
cd EvoLFuzzer

# Install dependencies
pip install -r requirements.txt
```

---

### 2. **Quick Start**
#### â–¶ Configure LLM API (Required)
Edit `llm_utils.py` and fill in your DeepSeek API Key:

```plain
openai.api_base = "https://api.deepseek.com/v1"
openai.api_key = ""  # Replace with your key
model = "deepseek-chat"
```

---

#### â–¶ Run Seed Generation
```plain
python main.py --method evolfuzzer --dataset HumanEval --epochs 5
```

---

#### â–¶ Run Fuzzing (Path Coverage Testing)
```plain
python test_coverage.py
```

---

## 3. **Parameter Description**
+ `--method`  
  Specify the fuzzing method to use. Available options:  
  `evolfuzzer`, `ea`, `ga`, `pso`, `aco`, `rma`.  
  Default: **evolfuzzer**
+ `--dataset`  
  Specify the benchmark dataset to test. Supported datasets:  
  `HumanEval`, `CWEval`, `MBPP`, `LLMSecEval`, `SecurityEval`.  
  Default: **HumanEval**
+ `--epochs`  
  Set the number of seed evolution rounds (iterations).  
  Can be any integer (e.g., 1â€“10), default: **5**

---

## ğŸ³ Running with Docker (Recommended âœ”)
To avoid executing potentially unsafe code, **it is strongly recommended to run this project in a Docker environment**.

---

## ğŸ“‚ Project Structure
```plain
EvoLFuzzer/
â”œâ”€â”€ ablation_study/                 # Ablation studies
â”œâ”€â”€ agent/                          # LLM agent and seed generation module
â”œâ”€â”€ datasets/                       # Benchmark datasets
â”œâ”€â”€ deepseek_coder/                 # Datasets generated by DeepSeekCoder
â”œâ”€â”€ draw_picture/                   # Convergence analysis visualizations
â”œâ”€â”€ draw_picture_bar_line/          # Bar/line chart visualizations
â”œâ”€â”€ origin_dataset_prompts/         # Original datasets and DeepSeekCoder outputs
â”œâ”€â”€ parameter_study/                # Parameter sensitivity experiments
â”œâ”€â”€ prompts/                        # Prompt templates
â”œâ”€â”€ results/                        # Path coverage results
â”œâ”€â”€ seed_results/                   # Seed generation results
â”œâ”€â”€ single_thread/                  # Single-threaded version
â”œâ”€â”€ static_dataset/                 # Vulnerability statistics
â”‚......
â”œâ”€â”€ create_seed_xxxx                # Seed creation by different methods
â”œâ”€â”€ fuzz_programmer_test_muti.py    # Multi-threaded fuzzing program
â””â”€â”€ llm_utils.py                    # LLM utility functions
```

---

## ğŸ“ˆ Experimental Results
The framework outputs:

+ Vulnerability reach statistics on security datasets
+ Path coverage comparison: EvoLFuzzer vs. baseline methods
+ Convergence curves and trend analysis of multi-round evolution
+ Parameter sensitivity analysis
+ Ablation study results

---

## ğŸ¤ Contribution
Contributions are welcome via Issues and Pull Requests!

Please ensure before submitting:

1. Follow the projectâ€™s code style
2. Add necessary tests
3. Update related documentation

---

## ğŸ™ Acknowledgements
+ Thanks to DeepSeek for providing powerful code generation model support
+ Thanks to maintainers of all benchmark datasets
+ Thanks to the open-source community for contributions and support

---

## ğŸ“¬ Contact
For questions or suggestions, please contact:

**ğŸ“§**** Email:**zhuyiwen@st.xatu.edu.cn

